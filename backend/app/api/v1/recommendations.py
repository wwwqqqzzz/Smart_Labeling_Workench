"""
RAGæ¨èAPIè·¯ç”±
"""
from fastapi import APIRouter, Query, HTTPException
from pydantic import BaseModel
from typing import Optional, List
import os
import httpx
from app.services.rag.rag_service import get_rag_recommender
from app.database import SessionLocal
from app.models import Conversation

router = APIRouter()

# å®Œæ•´çš„æ ‡å‡†åŒ–æ ‡ç­¾å®šä¹‰ï¼ˆåŒ…å«è¯¦ç»†è¯´æ˜ï¼‰
TAG_DEFINITIONS = {
    # è·¯çº¿ç›¸å…³
    "ä¸èµ°é«˜é€Ÿ": "è·¯çº¿å¯ç»è¿‡é«˜é€Ÿï¼Œä½†è´§ä¸»è¦æ±‚ä¸èµ°é«˜é€Ÿï¼Œå¸æœºåŒæ„ä¸èµ°",
    "æ— é«˜é€Ÿè´¹": "è´§ä¸»ä¸å‡ºé«˜é€Ÿè´¹ï¼Œå¸æœºåŒæ„ï¼Œä½†èµ°é«˜é€Ÿ",
    "éƒ¨åˆ†è¿‡è·¯/æ¡¥/èˆ¹/é«˜é€Ÿè´¹": "è´§ä¸»æ‰¿æ‹…éƒ¨åˆ†è¿‡è·¯/æ¡¥/èˆ¹/é«˜é€Ÿè´¹ï¼Œå¸æœºåŒæ„",
    "è¿‡è·¯/æ¡¥/èˆ¹/é«˜é€Ÿè´¹": "è´§ä¸»æ‰¿æ‹…å…¨éƒ¨è¿‡è·¯/æ¡¥/èˆ¹/é«˜é€Ÿè´¹",

    # è½¦è¾†å°ºå¯¸
    "è½¦å¢é•¿Xç±³": "å¸æœºæè¿°è‡ªå·±çš„è½¦å¢é•¿åº¦ï¼ˆå¦‚ï¼šè½¦å¢é•¿4.2ç±³ã€6.8ç±³ç­‰ï¼‰",
    "è½¦å®½Xç±³": "å¸æœºæè¿°è‡ªå·±çš„è½¦å¢å®½åº¦ï¼ˆå¦‚ï¼šè½¦å®½2ç±³ã€2.3ç±³ç­‰ï¼‰",
    "è½¦é«˜Xç±³": "å¸æœºæè¿°è‡ªå·±çš„è½¦å¢é«˜åº¦ï¼ˆå¦‚ï¼šè½¦é«˜2.2ç±³ã€2.5ç±³ç­‰ï¼‰",
    "è½¦å®¹é‡Xæ–¹": "å¸æœºæè¿°è‡ªå·±çš„è½¦å¯ä»¥è£…å¤šå°‘æ–¹ã€‚æ‹¼è½¦åœºæ™¯ä¸‹å‰©ä½™ç©ºé—´ä¸ç®—",
    "è½¦è½½é‡Xå¨": "å¸æœºæè¿°è‡ªå·±çš„è½¦å¯ä»¥è£…å¤šå°‘å¨",

    # è½¦å‹åˆ†ç±»
    "é¢åŒ…è½¦": "ä¼˜å…ˆæ ‡æ³¨æ˜ç¡®çš„è½¦å‹ï¼Œå¦‚æ— è¡¨è¿°ï¼Œå†æ ‡æ³¨éXXã€‚å¦‚ï¼šå¸æœºè¡¨è¾¾è‡ªå·±ä¸æ˜¯å¹³æ¿ï¼Œæ˜¯å¢è´§ï¼Œåº”è¯¥æ‰“æ ‡ä¸ºå¢è´§ï¼›å¸æœºè¡¨è¾¾è‡ªå·±ä¸æ˜¯å¹³æ¿ï¼Œæ²¡æœ‰è¯´å…·ä½“æ˜¯ä»€ä¹ˆè½¦å‹ï¼Œåº”è¯¥æ‰“æ ‡ä¸ºéå¹³æ¿",
    "é«˜æ ": "é«˜æ è½¦å‹",
    "å¢è´§": "å¢å¼è´§è½¦",
    "å¹³æ¿": "å¹³æ¿è½¦å‹",
    "ä¾ç»´æŸ¯": "ä¾ç»´æŸ¯è½¦å‹",
    "é£ç¿¼è½¦": "é£ç¿¼è½¦å‹ï¼ˆä¾§é—¨åƒç¿…è†€ä¸€æ ·æ‰“å¼€ï¼‰",
    "éå¹³æ¿": "å¸æœºæ˜ç¡®è¡¨ç¤ºä¸æ˜¯å¹³æ¿è½¦",
    "éé¢åŒ…è½¦": "å¸æœºæ˜ç¡®è¡¨ç¤ºä¸æ˜¯é¢åŒ…è½¦",
    "éé«˜æ ": "å¸æœºæ˜ç¡®è¡¨ç¤ºä¸æ˜¯é«˜æ è½¦",
    "éå¢è´§": "å¸æœºæ˜ç¡®è¡¨ç¤ºä¸æ˜¯å¢è´§è½¦",

    # å°¾æ¿ç›¸å…³
    "å°¾æ¿è½¦": "è½¦è¾†è£…æœ‰å°¾æ¿",
    "å°¾æ¿è´¹": "æœ‰å°¾æ¿ï¼Œä¸”å¸è´§åŒæ–¹åŒæ„äº†å°¾æ¿è´¹",
    "æ— å°¾æ¿": "è½¦è¾†æ²¡æœ‰å°¾æ¿",

    # è£…å¸ç›¸å…³
    "è£…å¸è´¹": "éœ€è¦å¸æœºè£…å¸ï¼Œä¸”å¸æœºè¡¨ç¤ºéœ€è¦è£…å¸è´¹ç”¨",
    "æ¬è¿è£…å¸": "éœ€è¦å¸æœºè‡ªå·±è£…å¸ï¼ˆå¸æœºè¦äº†æ¬è¿è´¹ï¼Œä½†è´§ä¸»æ‹’ç»ï¼Œå¸æœºè®¤å¯ä¸ç»™ä¹Ÿè¡Œï¼‰",
    "æ­æŠŠæ‰‹": "æœ‰äººè£…å¸ï¼Œå¸æœºéœ€è¦å¸®å¿™ï¼Œä¸”æ— æ¬è¿è´¹ç”¨ï¼Œæåˆ°è´¹ç”¨çš„å½’åˆ°è£…å¸è´¹",
    "ä¸æ¬è¿": "å¸æœºæ˜ç¡®æ‹’ç»ä¸å¸®å¿™æ¬è¿",

    # è·Ÿè½¦è¦æ±‚
    "è·Ÿè½¦Xäºº": "ä¸èƒ½è·Ÿè½¦ã€è·Ÿè½¦1äººã€è·Ÿè½¦2äººåŠä»¥ä¸Šï¼›æŒ‰ç…§å¸æœºæè¿°å¯è·Ÿè½¦çš„äººæ•°æ‰“æ ‡",

    # æ‹¼è½¦
    "æ‹¼è½¦å•": "å¸æœºè‡ªè¡Œæ‹¼è½¦æˆ–è€…å¯æ¥å—æ‹¼è½¦",

    # è£…å¸è¦æ±‚
    "Xè£…Xå¸": "è´§æºä¸ºXè£…Xå¸ï¼Œå¸æœºæ¥å—ï¼Œè´¹ç”¨æœªè°ˆæ‹¢ä¹Ÿç®—",

    # ä¾§é—¨ç±»å‹
    "ä¾§é—¨å•å¼€": "è‡³å°‘æœ‰ä¸€ä¾§å¯ä»¥å¼€ä¸€æ‰‡é—¨",
    "ä¾§é—¨åŒå¼€": "è‡³å°‘æœ‰ä¸€ä¾§å¯ä»¥å¼€ä¸¤æ‰‡é—¨",
    "ä¾§é—¨å…¨å¼€": "ä¾§è¾¹é—¨å¯ä»¥å…¨éƒ¨æ‰“å¼€",
    "ä¾§è¾¹æ ï¼Œä¾§é—¨å…¨å¼€": "ä¾§è¾¹é—¨å¯ä»¥å…¨éƒ¨æ‰“å¼€ï¼Œä½†æ˜¯é¡¶ä¸Šæœ‰æ æ†ï¼Œæ— æ³•æ‹†å¸",
    "åŒè¾¹ä¾§é—¨å…¨å¼€": "ä¸¤è¾¹çš„ä¾§é—¨éƒ½å¯ä»¥å…¨å¼€",
    "éä¾§å¼€é—¨": "ä¾§è¾¹é—¨ä¸èƒ½æ‰“å¼€ï¼Œæˆ–è€…å¸æœºä¸æ„¿æ„æ‰“å¼€ä¹Ÿç®—",

    # æ—¶é—´è¦æ±‚
    "æ˜æ—¥å¸": "æ˜å¤©å¸è´§",
    "æ˜æ—¥è£…å¸": "æ˜å¤©è£…å¸è´§",
    "å›ºå®š/ä¸Šç­æ—¶é—´è£…å¸": "åœ¨å·¥ä½œæ—¶é—´ï¼ˆ8:00-18:00ï¼‰è£…å¸",
    "å¤œé—´è¿è¾“": "åœ¨å¤œé—´ï¼ˆ18:00-æ¬¡æ—¥8:00ï¼‰è¿è¾“æˆ–è£…å¸",

    # è½¦è¾†åŠ¨åŠ›
    "æ–°èƒ½æº": "ç”µè½¦ä¹Ÿå±äºæ–°èƒ½æº",
    "æ²¹è½¦": "ç‡ƒæ²¹è½¦",

    # è½¦é—¨ç±»å‹
    "åŒå¼€é—¨": "åŒºåˆ«äºä¾§é—¨åŒå¼€ï¼ŒæŒ‡å°¾éƒ¨åŒå¼€é—¨",
    "éåŒå¼€é—¨": "å°¾éƒ¨ä¸æ˜¯åŒå¼€é—¨",

    # åº§ä½ç›¸å…³
    "æ— åº§è½¦": "è´§è¿ç‰ˆï¼Œæœ¬èº«æ²¡æœ‰åº§ä½ï¼›æˆ–è€…å®¢è¿ç‰ˆï¼Œåº§ä½éƒ½æ‹†äº†ï¼ŒæŠ˜å çš„ä¸ç®—",

    # è¾…åŠ©å·¥å…·
    "å°æ¨è½¦": "å¸æœºå½“ä¸‹æœ‰æ‰ç®—ï¼Œå¦‚æœè¯´è¦å›å®¶å–ï¼Œé‚£æ˜¯æ— å°æ¨è½¦",
    "æ— å°æ¨è½¦": "å¸æœºå½“ä¸‹æ²¡æœ‰å°æ¨è½¦",

    # è½¦é¡¶ç±»å‹
    "å¼€é¡¶è½¦å¢": "è½¦é¡¶å¯ä»¥å…¨éƒ¨æ‰“å¼€",
    "ä¸å¯å¼€é¡¶": "è½¦é¡¶ä¸èƒ½æ‰“å¼€",
    "ä¸å¯å…¨å¼€é¡¶": "é«˜æ è½¦ï¼Œæ»‘åŠ¨é›¨å¸ƒï¼Œé›¨å¸ƒå¯ä¸æ‹†ï¼Œå› æ­¤æœ‰ä¸€éƒ¨åˆ†æ— æ³•æ‰“å¼€",

    # é›¨å¸ƒç»³å­
    "é›¨å¸ƒ": "è½¦ä¸Šæœ‰é›¨å¸ƒã€é›¨æ£šéƒ½ç®—",
    "æ— é›¨å¸ƒ": "è½¦ä¸Šæ²¡æœ‰é›¨å¸ƒ",
    "æœ‰ç»³å­": "è½¦ä¸Šæœ‰ç»³å­ã€ç½‘å…œéƒ½ç®—",
    "æ— ç»³å­": "è½¦ä¸Šæ²¡æœ‰ç»³å­ï¼Œæˆ–è€…è½¦è¾†æ— æ³•ç”¨ç»³å­å›ºå®š",

    # è´¹ç”¨ç›¸å…³
    "è¿›å‡ºåœºè´¹": "å¦‚æœ‰æåŠï¼Œä¸”è´§ä¸»æ„¿æ„å‡ºå°±ç®—",
    "ç­‰å¾…è´¹": "æåŠç­‰å¾…è´¹ç”¨",
    "åœè½¦è´¹": "æåŠåœè½¦è´¹ç”¨",
}

ALL_TAGS = list(TAG_DEFINITIONS.keys())


class RecommendationRequest(BaseModel):
    """æ¨èè¯·æ±‚æ¨¡å‹"""
    conversation_id: Optional[int] = None
    text: Optional[str] = None
    top_k: int = Query(3, ge=1, le=10, description="è¿”å›æœ€ç›¸ä¼¼çš„Kä¸ªå¯¹è¯")


@router.post("/tags")
async def recommend_tags(request: RecommendationRequest):
    """
    åŸºäºç›¸ä¼¼å¯¹è¯æ¨èæ ‡ç­¾
    
    - **conversation_id**: å¯¹è¯IDï¼ˆå¦‚æœæä¾›ï¼Œè‡ªåŠ¨è·å–æ–‡æœ¬ï¼‰
    - **text**: å¯¹è¯æ–‡æœ¬ï¼ˆå¦‚æœä¸æä¾›conversation_idï¼Œåˆ™å¿…é¡»æä¾›textï¼‰
    - **top_k**: è¿”å›æœ€ç›¸ä¼¼çš„Kä¸ªå¯¹è¯
    """
    try:
        # è·å–å¯¹è¯æ–‡æœ¬
        text = request.text
        if request.conversation_id and not text:
            db = SessionLocal()
            try:
                conversation = db.query(Conversation).filter(
                    Conversation.id == request.conversation_id
                ).first()
                
                if not conversation:
                    raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")
                
                text = conversation.raw_text
            finally:
                db.close()
        
        if not text:
            raise HTTPException(status_code=400, detail="å¿…é¡»æä¾›conversation_idæˆ–text")
        
        # è·å–æ¨è
        recommender = get_rag_recommender()
        result = recommender.recommend_tags(
            conversation_text=text,
            top_k=request.top_k
        )
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        return {
            "success": False,
            "message": f"æ¨èå¤±è´¥: {str(e)}",
            "recommendations": [],
            "confidence": 0.0
        }


@router.post("/index/build")
async def build_index():
    """
    æ„å»ºæˆ–é‡å»ºå‘é‡ç´¢å¼•
    
    ä»æ‰€æœ‰å·²å®¡æ ¸çš„å¯¹è¯æ„å»ºå‘é‡ç´¢å¼•
    """
    try:
        db = SessionLocal()
        try:
            # è·å–æ‰€æœ‰å·²å®¡æ ¸çš„å¯¹è¯
            conversations = db.query(Conversation).filter(
                Conversation.status == 'approved'
            ).all()
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            conv_list = []
            for conv in conversations:
                conv_dict = {
                    'id': conv.id,
                    'raw_text': conv.raw_text,
                    'manual_tag': conv.manual_tag
                }
                conv_list.append(conv_dict)
            
            # æ„å»ºç´¢å¼•
            recommender = get_rag_recommender()
            result = recommender.build_vector_index(conv_list)
            
            return result
            
        finally:
            db.close()
            
    except Exception as e:
        return {
            "success": False,
            "message": f"æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}"
        }


@router.get("/index/stats")
async def get_index_stats():
    """è·å–å‘é‡ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
    try:
        recommender = get_rag_recommender()
        stats = recommender.get_index_stats()
        
        # è·å–æ•°æ®åº“ç»Ÿè®¡
        db = SessionLocal()
        try:
            total_conv = db.query(Conversation).count()
            approved_conv = db.query(Conversation).filter(
                Conversation.status == 'approved'
            ).count()
            
            stats['database'] = {
                'total_conversations': total_conv,
                'approved_conversations': approved_conv
            }
        finally:
            db.close()
        
        return {
            "success": True,
            "data": stats
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }


async def call_glm_api(prompt: str, max_tokens: int = 2000) -> dict:
    """
    è°ƒç”¨æ™ºè°±GLM APIè¿›è¡ŒAIåˆ†æ
    """
    from app.config import settings

    api_key = settings.GLM_API_KEY or os.getenv("GLM_API_KEY") or os.getenv("ZHIPU_API_KEY")
    if not api_key:
        print("âŒ [AIè°ƒç”¨] æœªé…ç½®GLM_API_KEY")
        return {"success": False, "error": "æœªé…ç½®GLM_API_KEY"}

    print(f"âœ… [AIè°ƒç”¨] å‡†å¤‡è°ƒç”¨GLM APIï¼Œprompté•¿åº¦: {len(prompt)}å­—ç¬¦")

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "glm-4-flash",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": max_tokens
                }
            )

            print(f"ğŸ“¡ [AIè°ƒç”¨] APIå“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                data = response.json()
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                print(f"âœ… [AIè°ƒç”¨] æˆåŠŸè·å–AIå“åº”ï¼Œå†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
                print(f"ğŸ“„ [AIå“åº”] å‰200å­—ç¬¦: {content[:200]}...")
                return {"success": True, "content": content}
            else:
                error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                print(f"âŒ [AIè°ƒç”¨] {error_msg}")
                return {
                    "success": False,
                    "error": error_msg
                }
    except Exception as e:
        print(f"âŒ [AIè°ƒç”¨] å¼‚å¸¸: {str(e)}")
        return {"success": False, "error": str(e)}


async def analyze_initial_tags_with_ai(conversation_text: str, initial_tags: list) -> dict:
    """
    ç¬¬ä¸€å±‚AIåˆ†æï¼šéªŒè¯åˆå§‹AIæ ‡ç­¾æ˜¯å¦åˆé€‚

    è¿”å›ï¼š{
        "appropriate_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],  # åˆé€‚çš„æ ‡ç­¾
        "inappropriate_tags": ["æ ‡ç­¾3"],  # ä¸åˆé€‚çš„æ ‡ç­¾
        "reasons": {"æ ‡ç­¾1": "åˆé€‚ç†ç”±", "æ ‡ç­¾3": "ä¸åˆé€‚ç†ç”±"}
    }
    """
    if not initial_tags:
        print("âš ï¸ [ç¬¬ä¸€å±‚AI] æ²¡æœ‰åˆå§‹AIæ ‡ç­¾éœ€è¦éªŒè¯")
        return {"appropriate_tags": [], "inappropriate_tags": [], "reasons": {}}

    print(f"ğŸ” [ç¬¬ä¸€å±‚AI] å¼€å§‹éªŒè¯åˆå§‹AIæ ‡ç­¾: {initial_tags}")

    # æ„å»ºæ ‡ç­¾å®šä¹‰è¯´æ˜
    tag_definitions_str = "\n".join([
        f"- {tag}: {TAG_DEFINITIONS.get(tag, 'æ— è¯´æ˜')}"
        for tag in initial_tags
    ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´§è¿å¯¹è¯æ ‡æ³¨ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å¸æœºä¸è´§ä¸»çš„å¯¹è¯å†…å®¹ï¼ŒéªŒè¯åˆå§‹AIæ¨èçš„æ ‡ç­¾æ˜¯å¦åˆé€‚ã€‚

## å¯¹è¯å†…å®¹ï¼š
{conversation_text}

## åˆå§‹AIæ¨èçš„æ ‡ç­¾ï¼š
{tag_definitions_str}

## ä½ çš„ä»»åŠ¡ï¼š
è¯·é€ä¸ªåˆ†ææ¯ä¸ªåˆå§‹AIæ ‡ç­¾ï¼Œåˆ¤æ–­æ˜¯å¦åˆé€‚ï¼Œå¹¶ç»™å‡ºç†ç”±ã€‚

## æ ‡ç­¾åˆ¤æ–­æ ‡å‡†ï¼š
- **åˆé€‚**ï¼šå¯¹è¯å†…å®¹æ˜ç¡®æåˆ°æˆ–æš—ç¤ºè¯¥æ ‡ç­¾æ‰€æè¿°çš„ç‰¹å¾
- **ä¸åˆé€‚**ï¼šå¯¹è¯å†…å®¹æœªæåŠã€ç›¸åã€æˆ–ä¸è¶³ä»¥æ”¯æŒè¯¥æ ‡ç­¾

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼‰ï¼š
{{
    "appropriate_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
    "inappropriate_tags": ["æ ‡ç­¾3"],
    "reasons": {{
        "æ ‡ç­¾1": "å¯¹è¯ä¸­æåˆ°xxxï¼Œç¬¦åˆè¯¥æ ‡ç­¾å®šä¹‰",
        "æ ‡ç­¾3": "å¯¹è¯ä¸­æœªæåŠxxxï¼Œä¸ç¬¦åˆè¯¥æ ‡ç­¾å®šä¹‰"
    }}
}}

è¯·åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"""

    result = await call_glm_api(prompt, max_tokens=1500)

    if not result.get("success"):
        print(f"âŒ [ç¬¬ä¸€å±‚AI] AIè°ƒç”¨å¤±è´¥: {result.get('error')}")
        return {"appropriate_tags": [], "inappropriate_tags": initial_tags, "reasons": {}}

    try:
        import json
        content = result["content"]

        # æå–JSONéƒ¨åˆ†ï¼ˆå¤„ç†å¯èƒ½çš„markdownä»£ç å—ï¼‰
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        analysis = json.loads(content)
        appropriate = analysis.get("appropriate_tags", [])
        inappropriate = analysis.get("inappropriate_tags", [])
        reasons = analysis.get("reasons", {})

        print(f"âœ… [ç¬¬ä¸€å±‚AI] éªŒè¯å®Œæˆ: {len(appropriate)}ä¸ªåˆé€‚, {len(inappropriate)}ä¸ªä¸åˆé€‚")
        print(f"   âœ“ åˆé€‚: {appropriate}")
        print(f"   âœ— ä¸åˆé€‚: {inappropriate}")

        return {
            "appropriate_tags": appropriate,
            "inappropriate_tags": inappropriate,
            "reasons": reasons
        }
    except Exception as e:
        print(f"âŒ [ç¬¬ä¸€å±‚AI] JSONè§£æå¤±è´¥: {str(e)}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰æ ‡ç­¾ä¸ºä¸åˆé€‚
        return {
            "appropriate_tags": [],
            "inappropriate_tags": initial_tags,
            "reasons": {},
            "parse_error": str(e)
        }


async def recommend_tags_from_conversation_with_ai(conversation_text: str, exclude_tags: list = None) -> dict:
    """
    ç¬¬äºŒå±‚AIåˆ†æï¼šæ·±å…¥åˆ†æå¯¹è¯å†…å®¹ï¼Œæ¨èåˆé€‚çš„æ ‡ç­¾

    è¿”å›ï¼š{
        "recommended_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
        "reasons": {"æ ‡ç­¾1": "æ¨èç†ç”±1", "æ ‡ç­¾2": "æ¨èç†ç”±2"}
    }
    """
    exclude_tags = exclude_tags or []
    print(f"ğŸ” [ç¬¬äºŒå±‚AI] å¼€å§‹åˆ†æå¯¹è¯å†…å®¹ï¼Œæ’é™¤æ ‡ç­¾: {exclude_tags}")

    # æ„å»ºæ‰€æœ‰æ ‡ç­¾å®šä¹‰
    all_tags_str = "\n".join([
        f"- {tag}: {TAG_DEFINITIONS[tag]}"
        for tag in TAG_DEFINITIONS.keys()
    ])

    exclude_tags_str = ", ".join(exclude_tags) if exclude_tags else "æ— "

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´§è¿å¯¹è¯æ ‡æ³¨ä¸“å®¶ã€‚è¯·æ·±å…¥åˆ†æä»¥ä¸‹å¸æœºä¸è´§ä¸»çš„å¯¹è¯å†…å®¹ï¼Œæ¨èåˆé€‚çš„æ ‡ç­¾ã€‚

## å¯¹è¯å†…å®¹ï¼š
{conversation_text}

## æ‰€æœ‰å¯ç”¨çš„æ ‡å‡†åŒ–æ ‡ç­¾åŠå…¶å®šä¹‰ï¼š
{all_tags_str}

## å·²æ’é™¤çš„æ ‡ç­¾ï¼ˆä¸éœ€è¦å†æ¬¡æ¨èï¼‰ï¼š
{exclude_tags_str}

## ä½ çš„ä»»åŠ¡ï¼š
æ ¹æ®å¯¹è¯å†…å®¹ï¼Œä»ä¸Šè¿°æ ‡ç­¾åˆ—è¡¨ä¸­é€‰æ‹©åˆé€‚çš„æ ‡ç­¾ã€‚ä¼˜å…ˆé€‰æ‹©æ˜ç¡®æåŠçš„ç‰¹å¾ã€‚

## æ ‡ç­¾é€‰æ‹©æ ‡å‡†ï¼š
1. å¯¹è¯ä¸­æ˜ç¡®æåˆ°çš„ç‰¹å¾ï¼ˆå¦‚è½¦å‹ã€å°ºå¯¸ã€è´¹ç”¨ç­‰ï¼‰
2. åŒæ–¹è¾¾æˆä¸€è‡´çš„è¦æ±‚æˆ–çº¦å®š
3. å¸æœºæˆ–è´§ä¸»æ˜ç¡®è¡¨ç¤ºçš„é™åˆ¶æˆ–æ¡ä»¶
4. ä¸è¦é€‰æ‹©å¯¹è¯ä¸­æœªæåŠçš„æ ‡ç­¾

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼‰ï¼š
{{
    "recommended_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "reasons": {{
        "æ ‡ç­¾1": "å¯¹è¯ä¸­å¸æœºæ˜ç¡®è¯´xxxï¼Œç¬¦åˆè¯¥æ ‡ç­¾å®šä¹‰",
        "æ ‡ç­¾2": "è´§ä¸»è¦æ±‚xxxï¼Œå¸æœºåŒæ„ï¼Œç¬¦åˆæ ‡ç­¾å®šä¹‰"
    }}
}}

è¯·åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"""

    result = await call_glm_api(prompt, max_tokens=2000)

    if not result.get("success"):
        print(f"âŒ [ç¬¬äºŒå±‚AI] AIè°ƒç”¨å¤±è´¥: {result.get('error')}")
        return {"recommended_tags": [], "reasons": {}}

    try:
        import json
        content = result["content"]

        # æå–JSONéƒ¨åˆ†
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        recommendation = json.loads(content)
        recommended = recommendation.get("recommended_tags", [])
        reasons = recommendation.get("reasons", {})

        print(f"âœ… [ç¬¬äºŒå±‚AI] åˆ†æå®Œæˆï¼Œæ¨èäº† {len(recommended)} ä¸ªæ ‡ç­¾: {recommended}")

        return {
            "recommended_tags": recommended,
            "reasons": reasons
        }
    except Exception as e:
        print(f"âŒ [ç¬¬äºŒå±‚AI] JSONè§£æå¤±è´¥: {str(e)}")
        return {"recommended_tags": [], "reasons": {}, "parse_error": str(e)}


@router.post("/ai/analyze")
async def ai_analyze_tags(request: RecommendationRequest):
    """
    ä¸‰å±‚AIæ·±åº¦åˆ†ææ¨èæ ‡ç­¾ï¼š

    ç¬¬ä¸€å±‚ï¼šAIåˆ†æåˆå§‹AIæ ‡ç­¾æ˜¯å¦åˆé€‚ï¼Œç»™å‡ºè¯¦ç»†ç†ç”±
    ç¬¬äºŒå±‚ï¼šAIæ·±å…¥åˆ†æå½“å‰å¯¹è¯å†…å®¹ï¼Œæ¨èåˆé€‚çš„æ ‡ç­¾
    ç¬¬ä¸‰å±‚ï¼šå‚è€ƒå†å²ç›¸ä¼¼å¯¹è¯ä½œä¸ºè¡¥å……
    """
    try:
        # è·å–å¯¹è¯æ–‡æœ¬å’Œåˆå§‹æ ‡ç­¾
        text = request.text
        conversation_id = request.conversation_id
        driver_tags = []

        if conversation_id and not text:
            db = SessionLocal()
            try:
                conversation = db.query(Conversation).filter(
                    Conversation.id == conversation_id
                ).first()

                if not conversation:
                    raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")

                text = conversation.raw_text

                # æå–åˆå§‹AIæ ‡ç­¾
                if conversation.driver_tag:
                    try:
                        import json
                        parsed = json.loads(conversation.driver_tag)
                        driver_tags = parsed if isinstance(parsed, list) else [parsed]
                        driver_tags = [t for t in driver_tags if t]
                    except:
                        if conversation.driver_tag:
                            driver_tags = [conversation.driver_tag]
            finally:
                db.close()

        if not text:
            raise HTTPException(status_code=400, detail="å¿…é¡»æä¾›conversation_idæˆ–text")

        # ========== ç¬¬ä¸€å±‚ï¼šAIåˆ†æåˆå§‹AIæ ‡ç­¾æ˜¯å¦åˆé€‚ ==========
        initial_analysis = await analyze_initial_tags_with_ai(text, driver_tags)

        # ========== ç¬¬äºŒå±‚ï¼šAIæ·±å…¥åˆ†æå¯¹è¯å†…å®¹ï¼Œæ¨èæ ‡ç­¾ ==========
        # æ’é™¤ç¬¬ä¸€å±‚ç¡®è®¤åˆé€‚çš„æ ‡ç­¾ï¼Œè®©AIæ¨èæ–°æ ‡ç­¾
        exclude_from_layer2 = initial_analysis.get("appropriate_tags", [])
        conversation_analysis = await recommend_tags_from_conversation_with_ai(text, exclude_tags=exclude_from_layer2)

        # ========== ç¬¬ä¸‰å±‚ï¼šå‚è€ƒå†å²ç›¸ä¼¼å¯¹è¯ ==========
        rag_recommender = get_rag_recommender()
        rag_result = rag_recommender.recommend_tags(
            conversation_text=text,
            top_k=10,
            min_similarity=0.3
        )

        rag_tags_with_reason = {}
        similar_conversations_enhanced = []

        if rag_result.get("success") and rag_result.get("similar_conversations"):
            # æ’é™¤å‰ä¸¤å±‚å·²æ¨èçš„æ ‡ç­¾
            existing_tags = set(initial_analysis.get("appropriate_tags", [])) | \
                           set(conversation_analysis.get("recommended_tags", []))

            for conv in rag_result["similar_conversations"]:
                conv_id = conv.get("conversation_id")
                similarity = conv.get("similarity", 0)

                db = SessionLocal()
                try:
                    conv_detail = db.query(Conversation).filter(
                        Conversation.id == conv_id
                    ).first()

                    if conv_detail and conv_detail.batch_id:
                        from app.models.import_batch import ImportBatch
                        batch = db.query(ImportBatch).filter(
                            ImportBatch.id == conv_detail.batch_id
                        ).first()

                        if conv.get("tags"):
                            for tag in conv["tags"]:
                                # åªæ¨èå°šæœªæ¨èçš„æ ‡ç­¾
                                if tag not in existing_tags and tag in TAG_DEFINITIONS:
                                    if tag not in rag_tags_with_reason:
                                        rag_tags_with_reason[tag] = []

                                    reason = f"ç›¸ä¼¼åº¦{round(similarity*100)}%"
                                    if batch:
                                        reason += f" - æ¥è‡ªæ‰¹æ¬¡: {batch.file_name}"
                                    reason += f" (å¯¹è¯#{conv_id})"

                                    rag_tags_with_reason[tag].append({
                                        "reason": reason,
                                        "similarity": similarity,
                                        "conversation_id": conv_id,
                                        "file_name": batch.file_name if batch else "æœªçŸ¥",
                                        "conversation_snippet": conv.get("text", "")[:100] + "..."
                                    })
                                    existing_tags.add(tag)  # é¿å…é‡å¤æ·»åŠ 
                finally:
                    db.close()

        # ========== åˆå¹¶ä¸‰å±‚æ¨èç»“æœ ==========
        all_recommendations = {}
        tag_details = {}

        # ç¬¬ä¸€å±‚ï¼šéªŒè¯åˆé€‚çš„åˆå§‹AIæ ‡ç­¾ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        for tag in initial_analysis.get("appropriate_tags", []):
            reason = initial_analysis.get("reasons", {}).get(tag, "åˆå§‹AIæ¨èï¼ŒAIéªŒè¯åˆé€‚")
            all_recommendations[tag] = {
                "score": 10,
                "source": "initial_ai_verified",
                "reason": f"âœ“ {reason}"
            }
            tag_details[tag] = all_recommendations[tag]

        # ç¬¬äºŒå±‚ï¼šä»å¯¹è¯å†…å®¹AIæ¨èçš„æ ‡ç­¾
        for tag in conversation_analysis.get("recommended_tags", []):
            reason = conversation_analysis.get("reasons", {}).get(tag, "AIä»å¯¹è¯å†…å®¹åˆ†ææ¨è")
            all_recommendations[tag] = {
                "score": 8,
                "source": "conversation_ai",
                "reason": reason
            }
            tag_details[tag] = all_recommendations[tag]

        # ç¬¬ä¸‰å±‚ï¼šå†å²ç›¸ä¼¼å¯¹è¯æ¨è
        for tag, reasons_list in rag_tags_with_reason.items():
            if reasons_list and isinstance(reasons_list, list):
                all_recommendations[tag] = {
                    "score": 5,
                    "source": "historical_similar",
                    "reason": reasons_list[0]["reason"]
                }
                tag_details[tag] = all_recommendations[tag]

        # æŒ‰æƒé‡æ’åº
        sorted_recommendations = sorted(
            all_recommendations.items(),
            key=lambda x: x[1]["score"],
            reverse=True
        )

        final_tags = [tag for tag, details in sorted_recommendations]

        # ========== æ™ºèƒ½è‡ªåŠ¨é€‰æ‹©é€»è¾‘ ==========
        auto_select_tags = []
        appropriate_initial_tags = initial_analysis.get("appropriate_tags", [])

        if appropriate_initial_tags:
            # å¦‚æœæœ‰éªŒè¯åˆé€‚çš„åˆå§‹AIæ ‡ç­¾ï¼Œè‡ªåŠ¨é€‰ä¸­è¿™äº›
            auto_select_tags = appropriate_initial_tags
            print(f"âœ… [è‡ªåŠ¨é€‰æ‹©] ä½¿ç”¨éªŒè¯åˆé€‚çš„åˆå§‹æ ‡ç­¾: {auto_select_tags}")
        else:
            # å¦‚æœåˆå§‹AIæ ‡ç­¾éƒ½ä¸åˆé€‚ï¼Œä½¿ç”¨ç¬¬äºŒå±‚AIæ¨èçš„æ ‡ç­¾
            recommended_conversation_tags = conversation_analysis.get("recommended_tags", [])
            if recommended_conversation_tags:
                auto_select_tags = recommended_conversation_tags
                print(f"âœ… [è‡ªåŠ¨é€‰æ‹©] åˆå§‹æ ‡ç­¾ä¸åˆé€‚ï¼Œä½¿ç”¨ç¬¬äºŒå±‚AIæ¨è: {auto_select_tags}")
            else:
                print(f"âš ï¸ [è‡ªåŠ¨é€‰æ‹©] æ²¡æœ‰å¯è‡ªåŠ¨é€‰æ‹©çš„æ ‡ç­¾")

        # æ„å»ºç›¸ä¼¼å¯¹è¯è¯¦ç»†ä¿¡æ¯
        similar_convs_details = []
        for tag, reasons_list in rag_tags_with_reason.items():
            if reasons_list and isinstance(reasons_list, list):
                similar_convs_details.extend(reasons_list[:1])

        print(f"ğŸ“Š [æœ€ç»ˆç»“æœ] æ€»å…±æ¨è {len(final_tags)} ä¸ªæ ‡ç­¾ï¼Œè‡ªåŠ¨é€‰æ‹© {len(auto_select_tags)} ä¸ª")

        # æ„å»ºå“åº”
        return {
            "success": True,
            "recommendations": final_tags,
            "tag_details": tag_details,
            "auto_select_tags": auto_select_tags,  # æ–°å¢ï¼šè‡ªåŠ¨é€‰æ‹©çš„æ ‡ç­¾
            "confidence": min(0.98, 0.7 + len(final_tags) * 0.03),
            "message": f"ä¸‰å±‚AIåˆ†æï¼šéªŒè¯åˆå§‹æ ‡ç­¾({len(initial_analysis.get('appropriate_tags', []))}ä¸ªåˆé€‚) + å¯¹è¯å†…å®¹åˆ†æ({len(conversation_analysis.get('recommended_tags', []))}ä¸ª) + å†å²ç›¸ä¼¼({len(rag_tags_with_reason)}ä¸ª)",
            "similar_conversations": similar_convs_details[:5],
            "initial_ai_tags": driver_tags,
            "initial_ai_analysis": {
                "appropriate": initial_analysis.get("appropriate_tags", []),
                "inappropriate": initial_analysis.get("inappropriate_tags", []),
                "reasons": initial_analysis.get("reasons", {})
            },
            "conversation_analysis": {
                "recommended": conversation_analysis.get("recommended_tags", []),
                "reasons": conversation_analysis.get("reasons", {})
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        return {
            "success": False,
            "message": f"æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}\n{traceback.format_exc()}",
            "recommendations": [],
            "confidence": 0.0
        }
